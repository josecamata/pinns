# learning_rate: 0.000806
# num_dense_layers: 6
# num_dense_nodes: 80
# activation:ReLU 
# batch_size: 32
# final loss: 0.04557395726442337
# Training Time: 103.08365035057068

# step, loss_train, loss_test, metrics_test
0.000000000000000000e+00 2.499767839908599854e-01 4.905505105853080750e-02 4.426547512412071228e-02 1.170426607131958008e-02 1.296871621161699295e-02 6.297795772552490234e-01 2.499767839908599854e-01 4.905505105853080750e-02 4.426547512412071228e-02 1.170426607131958008e-02 1.296871621161699295e-02 6.297795772552490234e-01
1.000000000000000000e+03 2.063811011612415314e-02 3.331810148665681481e-05 9.583800419932231307e-06 5.423973198048770428e-05 1.109876466216519475e-04 2.472771704196929932e-02 2.063811011612415314e-02 3.331810148665681481e-05 9.583800419932231307e-06 5.423973198048770428e-05 1.109876466216519475e-04 2.472771704196929932e-02
2.000000000000000000e+03 1.932928524911403656e-02 7.689309859415516257e-05 4.911798168905079365e-05 1.642194547457620502e-04 1.962126261787489057e-04 3.458343073725700378e-02 1.932928524911403656e-02 7.689309859415516257e-05 4.911798168905079365e-05 1.642194547457620502e-04 1.962126261787489057e-04 3.458343073725700378e-02
3.000000000000000000e+03 1.903441548347473145e-02 7.275875395862385631e-05 1.874663576018065214e-05 7.267588080139830709e-05 1.574619964230805635e-04 4.293245822191238403e-02 1.903441548347473145e-02 7.275875395862385631e-05 1.874663576018065214e-05 7.267588080139830709e-05 1.574619964230805635e-04 4.293245822191238403e-02
4.000000000000000000e+03 2.094238251447677612e-02 1.182470514322631061e-04 1.276330476684961468e-05 8.333304867846891284e-05 2.344517706660553813e-04 3.382394462823867798e-02 2.094238251447677612e-02 1.182470514322631061e-04 1.276330476684961468e-05 8.333304867846891284e-05 2.344517706660553813e-04 3.382394462823867798e-02
5.000000000000000000e+03 1.812526769936084747e-02 1.105623887269757688e-04 4.138617441640235484e-05 1.701897417660802603e-04 2.764988166745752096e-04 3.481480851769447327e-02 1.812526769936084747e-02 1.105623887269757688e-04 4.138617441640235484e-05 1.701897417660802603e-04 2.764988166745752096e-04 3.481480851769447327e-02
6.000000000000000000e+03 2.041378244757652283e-02 1.122805624618194997e-04 3.031617416127119213e-05 5.224941924097947776e-05 2.070657501462846994e-04 4.981416836380958557e-02 2.041378244757652283e-02 1.122805624618194997e-04 3.031617416127119213e-05 5.224941924097947776e-05 2.070657501462846994e-04 4.981416836380958557e-02
7.000000000000000000e+03 3.191902488470077515e-02 1.374171115458011627e-04 1.139121814048849046e-04 4.912672447971999645e-04 2.492098428774625063e-04 4.518410563468933105e-02 3.191902488470077515e-02 1.374171115458011627e-04 1.139121814048849046e-04 4.912672447971999645e-04 2.492098428774625063e-04 4.518410563468933105e-02
8.000000000000000000e+03 2.105443738400936127e-02 7.770178490318357944e-05 4.123698090552352369e-05 9.948142542270943522e-05 2.012605982599779963e-04 2.898944728076457977e-02 2.105443738400936127e-02 7.770178490318357944e-05 4.123698090552352369e-05 9.948142542270943522e-05 2.012605982599779963e-04 2.898944728076457977e-02
9.000000000000000000e+03 1.893670111894607544e-02 1.610818981134798378e-05 4.223319592711050063e-06 5.975547901471145451e-05 1.223877043230459094e-04 3.162163868546485901e-02 1.893670111894607544e-02 1.610818981134798378e-05 4.223319592711050063e-06 5.975547901471145451e-05 1.223877043230459094e-04 3.162163868546485901e-02
1.000000000000000000e+04 1.627106219530105591e-02 5.122449510963633657e-05 2.934242183982860297e-05 8.813406566332560033e-06 2.016651160374749452e-05 4.455715417861938477e-02 1.627106219530105591e-02 5.122449510963633657e-05 2.934242183982860297e-05 8.813406566332560033e-06 2.016651160374749452e-05 4.455715417861938477e-02
