# learning_rate: 0.000283
# num_dense_layers: 6
# num_dense_nodes: 30
# activation:ReLU 
# batch_size: 32
# final loss: 0.06093768775463104
# Training Time: 96.20334577560425
# Best Step: 5000

# step, loss_train, loss_test, metrics_test
0.000000000000000000e+00 8.947008103132247925e-02 4.635273944586515427e-04 1.741050073178485036e-04 1.446675742045044899e-04 6.274060433497652411e-05 1.361190527677536011e-01 8.947008103132247925e-02 4.635273944586515427e-04 1.741050073178485036e-04 1.446675742045044899e-04 6.274060433497652411e-05 1.361190527677536011e-01
1.000000000000000000e+03 2.032687887549400330e-02 1.191756819025613368e-04 1.261427860299590975e-05 4.778050424647517502e-05 7.924492092570289969e-05 5.210549384355545044e-02 2.032687887549400330e-02 1.191756819025613368e-04 1.261427860299590975e-05 4.778050424647517502e-05 7.924492092570289969e-05 5.210549384355545044e-02
2.000000000000000000e+03 2.721572667360305786e-02 6.031563316355459392e-05 1.538740434625651687e-05 9.621546632843092084e-05 1.427205424988642335e-04 3.869212418794631958e-02 2.721572667360305786e-02 6.031563316355459392e-05 1.538740434625651687e-05 9.621546632843092084e-05 1.427205424988642335e-04 3.869212418794631958e-02
3.000000000000000000e+03 2.383308298885822296e-02 1.144699417636729777e-04 5.425131166703067720e-05 1.317730784649029374e-04 3.453633689787238836e-04 4.227221012115478516e-02 2.383308298885822296e-02 1.144699417636729777e-04 5.425131166703067720e-05 1.317730784649029374e-04 3.453633689787238836e-04 4.227221012115478516e-02
4.000000000000000000e+03 2.041764371097087860e-02 1.194118522107601166e-04 9.597398275218438357e-06 7.036816532490774989e-05 1.626522280275821686e-04 5.838911607861518860e-02 2.041764371097087860e-02 1.194118522107601166e-04 9.597398275218438357e-06 7.036816532490774989e-05 1.626522280275821686e-04 5.838911607861518860e-02
5.000000000000000000e+03 2.011654153466224670e-02 1.876346213975921273e-04 1.281346794712590054e-05 4.882173016085289419e-05 2.252168487757444382e-04 4.034665971994400024e-02 2.011654153466224670e-02 1.876346213975921273e-04 1.281346794712590054e-05 4.882173016085289419e-05 2.252168487757444382e-04 4.034665971994400024e-02
6.000000000000000000e+03 2.067776024341583252e-02 1.263265439774841070e-04 2.070712753265979700e-06 1.004686273518018425e-04 2.656662545632570982e-04 4.865402355790138245e-02 2.067776024341583252e-02 1.263265439774841070e-04 2.070712753265979700e-06 1.004686273518018425e-04 2.656662545632570982e-04 4.865402355790138245e-02
7.000000000000000000e+03 1.879183761775493622e-02 1.204710715683177114e-04 1.353308744000969455e-05 5.942474308540113270e-05 1.298160495935007930e-04 4.983594641089439392e-02 1.879183761775493622e-02 1.204710715683177114e-04 1.353308744000969455e-05 5.942474308540113270e-05 1.298160495935007930e-04 4.983594641089439392e-02
8.000000000000000000e+03 2.153487503528594971e-02 2.502718998584896326e-04 2.005792521231342107e-05 9.383394353790208697e-05 2.145713224308565259e-04 5.345895141363143921e-02 2.153487503528594971e-02 2.502718998584896326e-04 2.005792521231342107e-05 9.383394353790208697e-05 2.145713224308565259e-04 5.345895141363143921e-02
9.000000000000000000e+03 2.166495844721794128e-02 1.985245908144861460e-04 2.195561319240368903e-05 5.983639493933878839e-05 3.230060974601656199e-04 6.670789420604705811e-02 2.166495844721794128e-02 1.985245908144861460e-04 2.195561319240368903e-05 5.983639493933878839e-05 3.230060974601656199e-04 6.670789420604705811e-02
1.000000000000000000e+04 1.905716583132743835e-02 1.411944831488654017e-04 2.622922465889132582e-06 2.234488601970952004e-05 1.779763988452032208e-04 7.202303409576416016e-02 1.905716583132743835e-02 1.411944831488654017e-04 2.622922465889132582e-06 2.234488601970952004e-05 1.779763988452032208e-04 7.202303409576416016e-02
