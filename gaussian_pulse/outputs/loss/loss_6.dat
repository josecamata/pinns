# learning_rate: 0.04744052775564958
# num_dense_layers: 8
# num_dense_nodes: 43
# activation:sigmoid 
# batch_size: 32
# final loss: 0.21919098496437073
# Training Time: 111.65038228034973
# Best Step: 2000

# step, loss_train, loss_test, metrics_test
0.000000000000000000e+00 3.137816451115327254e-09 2.296721041202545166e-01 2.296714037656784058e-01 2.296684235334396362e-01 2.296683639287948608e-01 2.138792419433593750e+01 2.539509491583658018e-09 2.296721041202545166e-01 2.296714037656784058e-01 2.296684235334396362e-01 2.296683639287948608e-01 2.138792419433593750e+01
1.000000000000000000e+03 4.923682667227211388e-15 3.608255938161164522e-04 3.608256520237773657e-04 3.608255356084555387e-04 3.608255647122859955e-04 2.753378152847290039e-01 2.474845830452800967e-15 3.608257102314382792e-04 3.608257102314382792e-04 3.608255938161164522e-04 3.608256229199469090e-04 2.753378152847290039e-01
2.000000000000000000e+03 4.205381125211715698e-02 1.042024581693112850e-03 6.584897055290639400e-04 6.574859144166111946e-04 9.891990339383482933e-04 1.774995177984237671e-01 3.834426030516624451e-02 1.042024698108434677e-03 6.584898801520466805e-04 6.574859726242721081e-04 9.891992667689919472e-04 1.774995326995849609e-01
3.000000000000000000e+03 4.337919319542457575e-14 3.608851984608918428e-04 3.608852857723832130e-04 3.608851984608918428e-04 3.608850238379091024e-04 2.753375470638275146e-01 4.062766426450939439e-14 3.608851402532309294e-04 3.608851984608918428e-04 3.608851402532309294e-04 3.608849656302481890e-04 2.753375470638275146e-01
4.000000000000000000e+03 1.398384446432698880e-12 3.825309395324438810e-04 3.825309395324438810e-04 3.825305902864784002e-04 3.825304447673261166e-04 2.752839624881744385e-01 1.709214042001916400e-12 3.825309395324438810e-04 3.825309395324438810e-04 3.825305902864784002e-04 3.825304447673261166e-04 2.752839624881744385e-01
5.000000000000000000e+03 1.036343625119510309e-14 3.608361294027417898e-04 3.608361876104027033e-04 3.608361294027417898e-04 3.608360420912504196e-04 2.753377556800842285e-01 7.279868710890341488e-15 3.608361294027417898e-04 3.608361876104027033e-04 3.608361294027417898e-04 3.608360420912504196e-04 2.753377556800842285e-01
6.000000000000000000e+03 8.959726474741698532e-15 3.608286788221448660e-04 3.608286788221448660e-04 3.608286788221448660e-04 3.608286788221448660e-04 2.753378152847290039e-01 6.295697317827307279e-15 3.608287370298057795e-04 3.608287370298057795e-04 3.608287370298057795e-04 3.608286788221448660e-04 2.753378152847290039e-01
7.000000000000000000e+03 2.783331074087419684e-17 3.608250117395073175e-04 3.608250117395073175e-04 3.608250117395073175e-04 3.608250117395073175e-04 2.753378152847290039e-01 1.963384817735658279e-17 3.608250117395073175e-04 3.608250117395073175e-04 3.608250117395073175e-04 3.608250117395073175e-04 2.753378152847290039e-01
8.000000000000000000e+03 1.198416293616621304e-17 3.608260012697428465e-04 3.608260012697428465e-04 3.608260012697428465e-04 3.608260303735733032e-04 2.753378152847290039e-01 8.450836961408191294e-18 3.608259430620819330e-04 3.608259430620819330e-04 3.608259430620819330e-04 3.608259721659123898e-04 2.753378152847290039e-01
9.000000000000000000e+03 4.900731214334225100e-18 3.608263505157083273e-04 3.608263505157083273e-04 3.608263505157083273e-04 3.608263796195387840e-04 2.753378152847290039e-01 3.455655576395670697e-18 3.608263505157083273e-04 3.608263505157083273e-04 3.608263505157083273e-04 3.608263796195387840e-04 2.753378152847290039e-01
1.000000000000000000e+04 1.600684958169606538e-18 3.608249535318464041e-04 3.608249535318464041e-04 3.608249535318464041e-04 3.608249826356768608e-04 2.753378152847290039e-01 1.128684223515849774e-18 3.608249535318464041e-04 3.608249535318464041e-04 3.608249535318464041e-04 3.608249826356768608e-04 2.753378152847290039e-01
