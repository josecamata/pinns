# learning_rate: 0.004158800468954421
# num_dense_layers: 10
# num_dense_nodes: 120
# activation:tanh 
# batch_size: 32
# final loss: 0.2723032534122467
# Training Time: 187.8976390361786
# Best Step: 1000

# step, loss_train, loss_test, metrics_test
0.000000000000000000e+00 9.214833259582519531e+00 1.751550473272800446e-02 5.406919121742248535e-02 2.573000453412532806e-02 1.978164538741111755e-03 3.489903926849365234e+00 5.512301445007324219e+00 1.751550473272800446e-02 5.406919121742248535e-02 2.573000453412532806e-02 1.978164538741111755e-03 3.489903926849365234e+00
1.000000000000000000e+03 1.881171949207782745e-03 4.090609436389058828e-04 3.331032057758420706e-04 2.802305971272289753e-04 3.460592997726052999e-04 2.697110474109649658e-01 1.223753788508474827e-03 4.090609436389058828e-04 3.331032057758420706e-04 2.802305971272289753e-04 3.460592997726052999e-04 2.697110474109649658e-01
2.000000000000000000e+03 1.837793970480561256e-03 4.112455935683101416e-04 3.316225484013557434e-04 2.998633135575801134e-04 3.789317561313509941e-04 2.701661288738250732e-01 1.066034077666699886e-03 4.112455935683101416e-04 3.316225484013557434e-04 2.998633135575801134e-04 3.789317561313509941e-04 2.701661288738250732e-01
3.000000000000000000e+03 2.718313044169917703e-05 3.643877280410379171e-04 3.571748966351151466e-04 3.545105864759534597e-04 3.636788460426032543e-04 2.749004662036895752e-01 1.069367772288387641e-05 3.643877280410379171e-04 3.571748966351151466e-04 3.545105864759534597e-04 3.636788460426032543e-04 2.749004662036895752e-01
4.000000000000000000e+03 1.845747465267777443e-03 3.965350333601236343e-04 3.308188461232930422e-04 3.063805052079260349e-04 3.784697037190198898e-04 2.709386050701141357e-01 8.590107318013906479e-04 3.965350333601236343e-04 3.308188461232930422e-04 3.063805052079260349e-04 3.784697037190198898e-04 2.709386050701141357e-01
5.000000000000000000e+03 8.558175759389996529e-04 3.656701301224529743e-04 3.287069266662001610e-04 3.171358257532119751e-04 3.612286818679422140e-04 2.725371718406677246e-01 3.476327401585876942e-04 3.656701301224529743e-04 3.287069266662001610e-04 3.171358257532119751e-04 3.612286818679422140e-04 2.725371718406677246e-01
6.000000000000000000e+03 1.523964149319212424e-10 3.608672413975000381e-04 3.608467231970280409e-04 3.608489350881427526e-04 3.608699189499020576e-04 2.753362655639648438e-01 7.910540278377453660e-11 3.608672413975000381e-04 3.608467231970280409e-04 3.608489350881427526e-04 3.608699189499020576e-04 2.753362655639648438e-01
7.000000000000000000e+03 3.897399070140750155e-11 2.975823881570249796e-04 2.975832321681082249e-04 2.975795650854706764e-04 2.975772076752036810e-04 2.759074866771697998e-01 4.776113966126205312e-12 2.975823881570249796e-04 2.975832321681082249e-04 2.975795650854706764e-04 2.975772076752036810e-04 2.759074866771697998e-01
8.000000000000000000e+03 1.217087265104055405e-04 3.788113535847514868e-04 3.615849709603935480e-04 3.575968730729073286e-04 3.788928152061998844e-04 2.737289965152740479e-01 6.054286859580315650e-05 3.788113535847514868e-04 3.615849709603935480e-04 3.575968730729073286e-04 3.788928152061998844e-04 2.737289965152740479e-01
9.000000000000000000e+03 2.119161068847574825e-09 3.609203849919140339e-04 3.608519618865102530e-04 3.608467231970280409e-04 3.609121777117252350e-04 2.753320634365081787e-01 8.341590196359049969e-10 3.609203849919140339e-04 3.608519618865102530e-04 3.608467231970280409e-04 3.609121777117252350e-04 2.753320634365081787e-01
1.000000000000000000e+04 1.624310061454359300e-12 3.608275437727570534e-04 3.608280676417052746e-04 3.608279512263834476e-04 3.608266415540128946e-04 2.753378748893737793e-01 8.038922717605589963e-13 3.608275437727570534e-04 3.608280676417052746e-04 3.608279512263834476e-04 3.608266415540128946e-04 2.753378748893737793e-01
