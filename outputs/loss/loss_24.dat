# learning_rate: 0.0001
# num_dense_layers: 8
# num_dense_nodes: 70
# activation:ReLU 
# batch_size: 32
# final loss: 0.20110471546649933
# Training Time: 86.69629502296448
# Best Step: 5000

# step, loss_train, loss_test, metrics_test
0.000000000000000000e+00 3.231259882450103760e-01 1.343671465292572975e-03 3.484327171463519335e-04 1.472101866966113448e-04 2.733407600317150354e-04 4.690479636192321777e-01 3.231259882450103760e-01 1.343671465292572975e-03 3.484327171463519335e-04 1.472101866966113448e-04 2.733407600317150354e-04 4.690479636192321777e-01
1.000000000000000000e+03 2.341105975210666656e-02 4.355515411589294672e-04 2.085993328364565969e-04 1.854178553912788630e-04 3.532363043632358313e-04 2.388466745615005493e-01 2.341105975210666656e-02 4.355515411589294672e-04 2.085993328364565969e-04 1.854178553912788630e-04 3.532363043632358313e-04 2.388466745615005493e-01
2.000000000000000000e+03 3.779850900173187256e-02 1.996927021536976099e-04 1.330162922386080027e-04 2.187888603657484055e-04 2.637894358485937119e-04 1.721812933683395386e-01 3.779850900173187256e-02 1.996927021536976099e-04 1.330162922386080027e-04 2.187888603657484055e-04 2.637894358485937119e-04 1.721812933683395386e-01
3.000000000000000000e+03 3.183322399854660034e-02 1.676984102232381701e-04 9.692325693322345614e-05 1.406597002642229199e-04 1.575696660438552499e-04 1.817817986011505127e-01 3.183322399854660034e-02 1.676984102232381701e-04 9.692325693322345614e-05 1.406597002642229199e-04 1.575696660438552499e-04 1.817817986011505127e-01
4.000000000000000000e+03 3.749007731676101685e-02 1.719424471957609057e-04 1.050697901519015431e-04 1.924506796058267355e-04 2.177796995965763927e-04 1.737511008977890015e-01 3.749007731676101685e-02 1.719424471957609057e-04 1.050697901519015431e-04 1.924506796058267355e-04 2.177796995965763927e-04 1.737511008977890015e-01
5.000000000000000000e+03 3.620174899697303772e-02 1.794786512618884444e-04 1.590292959008365870e-04 1.917420595418661833e-04 2.007954026339575648e-04 1.641719192266464233e-01 3.620174899697303772e-02 1.794786512618884444e-04 1.590292959008365870e-04 1.917420595418661833e-04 2.007954026339575648e-04 1.641719192266464233e-01
6.000000000000000000e+03 3.023337386548519135e-02 2.287862880621105433e-04 1.231818605447188020e-04 2.220546157332137227e-04 2.654546115081757307e-04 1.852145493030548096e-01 3.023337386548519135e-02 2.287862880621105433e-04 1.231818605447188020e-04 2.220546157332137227e-04 2.654546115081757307e-04 1.852145493030548096e-01
7.000000000000000000e+03 2.982518263161182404e-02 2.187748905271291733e-04 1.049522761604748666e-04 1.939040957950055599e-04 2.669219102244824171e-04 1.797398924827575684e-01 2.982518263161182404e-02 2.187748905271291733e-04 1.049522761604748666e-04 1.939040957950055599e-04 2.669219102244824171e-04 1.797398924827575684e-01
8.000000000000000000e+03 2.967782132327556610e-02 1.826752704801037908e-04 8.425881969742476940e-05 1.687374460743740201e-04 2.094009396387264132e-04 1.909404844045639038e-01 2.967782132327556610e-02 1.826752704801037908e-04 8.425881969742476940e-05 1.687374460743740201e-04 2.094009396387264132e-04 1.909404844045639038e-01
9.000000000000000000e+03 2.774283103644847870e-02 2.488601894583553076e-04 1.040190327330492437e-04 1.788680237950757146e-04 2.609376097097992897e-04 1.941644549369812012e-01 2.774283103644847870e-02 2.488601894583553076e-04 1.040190327330492437e-04 1.788680237950757146e-04 2.609376097097992897e-04 1.941644549369812012e-01
1.000000000000000000e+04 2.421496249735355377e-02 2.403346152277663350e-04 1.315856061410158873e-04 2.319478953722864389e-04 2.739907358773052692e-04 2.052719891071319580e-01 2.421496249735355377e-02 2.403346152277663350e-04 1.315856061410158873e-04 2.319478953722864389e-04 2.739907358773052692e-04 2.052719891071319580e-01
