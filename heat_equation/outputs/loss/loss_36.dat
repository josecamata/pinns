# learning_rate: 0.005
# num_dense_layers: 5
# num_dense_nodes: 80
# activation:sigmoid 
# batch_size: 32
# final loss: 0.7889569997787476
# Training Time: 78.00630164146423
# Best Step: 10000

# step, loss_train, loss_test, metrics_test
0.000000000000000000e+00 5.706841648134286515e-07 1.612381362915039062e+01 2.801461696624755859e+00 1.866976357996463776e-02 1.866745710372924805e+00 4.669106960296630859e+00 5.706841648134286515e-07 1.612381362915039062e+01 2.801461696624755859e+00 1.866976357996463776e-02 1.866745710372924805e+00 4.669106960296630859e+00
1.000000000000000000e+03 8.345484733581542969e-02 2.160123586654663086e-01 4.808829128742218018e-01 2.774521135506802239e-06 3.635073304176330566e-01 3.939152359962463379e-01 8.345484733581542969e-02 2.160123586654663086e-01 4.808829128742218018e-01 2.774521135506802239e-06 3.635073304176330566e-01 3.939152359962463379e-01
2.000000000000000000e+03 3.784037753939628601e-02 1.984616070985794067e-01 4.008565545082092285e-01 1.503204885011655279e-06 3.056060671806335449e-01 2.251275181770324707e-01 3.784037753939628601e-02 1.984616070985794067e-01 4.008565545082092285e-01 1.503204885011655279e-06 3.056060671806335449e-01 2.251275181770324707e-01
3.000000000000000000e+03 2.482459321618080139e-02 2.403107732534408569e-01 3.620317876338958740e-01 1.254753351531689987e-05 2.794192433357238770e-01 1.652642786502838135e-01 2.482459321618080139e-02 2.403107732534408569e-01 3.620317876338958740e-01 1.254753351531689987e-05 2.794192433357238770e-01 1.652642786502838135e-01
4.000000000000000000e+03 3.387134149670600891e-02 2.472039759159088135e-01 3.411699533462524414e-01 6.885853053972823545e-06 2.630383968353271484e-01 1.221316084265708923e-01 3.387134149670600891e-02 2.472039759159088135e-01 3.411699533462524414e-01 6.885853053972823545e-06 2.630383968353271484e-01 1.221316084265708923e-01
5.000000000000000000e+03 3.731676191091537476e-02 1.155102476477622986e-01 4.053803086280822754e-01 1.396825882693519816e-05 3.126361668109893799e-01 1.331059783697128296e-01 3.731676191091537476e-02 1.155102476477622986e-01 4.053803086280822754e-01 1.396825882693519816e-05 3.126361668109893799e-01 1.331059783697128296e-01
6.000000000000000000e+03 3.424367681145668030e-02 1.367460489273071289e-01 4.174333810806274414e-01 2.489286089257802814e-05 3.219192922115325928e-01 1.348878443241119385e-01 3.424367681145668030e-02 1.367460489273071289e-01 4.174333810806274414e-01 2.489286089257802814e-05 3.219192922115325928e-01 1.348878443241119385e-01
7.000000000000000000e+03 3.022361733019351959e-02 1.575047224760055542e-01 3.503377437591552734e-01 3.381025862836395390e-06 2.730753719806671143e-01 8.538597822189331055e-02 3.022361733019351959e-02 1.575047224760055542e-01 3.503377437591552734e-01 3.381025862836395390e-06 2.730753719806671143e-01 8.538597822189331055e-02
8.000000000000000000e+03 3.220522031188011169e-02 2.430185377597808838e-01 3.167995810508728027e-01 3.119776465609902516e-06 2.444464266300201416e-01 6.990408152341842651e-02 3.220522031188011169e-02 2.430185377597808838e-01 3.167995810508728027e-01 3.119776465609902516e-06 2.444464266300201416e-01 6.990408152341842651e-02
9.000000000000000000e+03 5.615085363388061523e-02 9.502160549163818359e-02 3.385354876518249512e-01 2.852538614206423517e-07 2.578048706054687500e-01 7.587379962205886841e-02 5.615085363388061523e-02 9.502160549163818359e-02 3.385354876518249512e-01 2.852538614206423517e-07 2.578048706054687500e-01 7.587379962205886841e-02
1.000000000000000000e+04 4.873034358024597168e-02 7.010541856288909912e-02 3.396650552749633789e-01 4.945053433402790688e-07 2.431773245334625244e-01 8.727834373712539673e-02 4.873034358024597168e-02 7.010541856288909912e-02 3.396650552749633789e-01 4.945053433402790688e-07 2.431773245334625244e-01 8.727834373712539673e-02
