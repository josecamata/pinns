# learning_rate: 0.0023574384229405653
# num_dense_layers: 7
# num_dense_nodes: 120
# activation:sin 
# batch_size: 32
# final loss: 0.37454476952552795
# Training Time: 140.83798623085022
# Best Step: 10000

# step, loss_train, loss_test, metrics_test
0.000000000000000000e+00 4.861547946929931641e+00 4.159299850463867188e+01 2.023105770349502563e-01 2.912356751039624214e-03 2.781602442264556885e-01 5.760411173105239868e-02 4.687584877014160156e+00 4.159299850463867188e+01 2.023105770349502563e-01 2.912356751039624214e-03 2.781602442264556885e-01 5.760411173105239868e-02
1.000000000000000000e+03 9.243083745241165161e-02 2.524575293064117432e-01 4.847970306873321533e-01 1.720473278510326054e-06 3.550948500633239746e-01 3.319889008998870850e-01 3.374510258436203003e-02 2.524575293064117432e-01 4.847970306873321533e-01 1.720473278510326054e-06 3.550948500633239746e-01 3.319889008998870850e-01
2.000000000000000000e+03 7.064618915319442749e-02 8.915625512599945068e-02 3.598363995552062988e-01 2.612708613014547154e-06 2.769168019294738770e-01 1.854310482740402222e-01 2.128571644425392151e-02 8.915625512599945068e-02 3.598363995552062988e-01 2.612708613014547154e-06 2.769168019294738770e-01 1.854310482740402222e-01
3.000000000000000000e+03 1.013466492295265198e-01 2.676787674427032471e-01 2.529920935630798340e-01 2.245075847895350307e-06 1.942172050476074219e-01 1.144775971770286560e-01 1.221653632819652557e-02 2.676787674427032471e-01 2.529920935630798340e-01 2.245075847895350307e-06 1.942172050476074219e-01 1.144775971770286560e-01
4.000000000000000000e+03 5.371188744902610779e-02 1.279151141643524170e-01 1.610524207353591919e-01 2.605743929962045513e-06 1.243454143404960632e-01 1.192383095622062683e-01 1.492046844214200974e-02 1.279151141643524170e-01 1.610524207353591919e-01 2.605743929962045513e-06 1.243454143404960632e-01 1.192383095622062683e-01
5.000000000000000000e+03 5.094938725233078003e-02 1.503985226154327393e-01 1.290664374828338623e-01 5.950372724328190088e-06 8.623427152633666992e-02 1.088322252035140991e-01 1.163819059729576111e-02 1.503985226154327393e-01 1.290664374828338623e-01 5.950372724328190088e-06 8.623427152633666992e-02 1.088322252035140991e-01
6.000000000000000000e+03 6.437844783067703247e-02 3.287467956542968750e-01 1.310886293649673462e-01 1.677981526881922036e-05 5.520452558994293213e-02 8.235402405261993408e-02 6.028128787875175476e-03 3.287467956542968750e-01 1.310886293649673462e-01 1.677981526881922036e-05 5.520452558994293213e-02 8.235402405261993408e-02
7.000000000000000000e+03 1.157776191830635071e-01 1.722803413867950439e-01 3.755614459514617920e-01 9.052887435245793313e-06 2.922280430793762207e-01 4.366431236267089844e-01 5.230575427412986755e-02 1.722803413867950439e-01 3.755614459514617920e-01 9.052887435245793313e-06 2.922280430793762207e-01 4.366431236267089844e-01
8.000000000000000000e+03 5.337382480502128601e-02 8.974426984786987305e-02 1.624408364295959473e-01 3.196130637661553919e-05 1.027343571186065674e-01 1.595224887132644653e-01 2.604681067168712616e-02 8.974426984786987305e-02 1.624408364295959473e-01 3.196130637661553919e-05 1.027343571186065674e-01 1.595224887132644653e-01
9.000000000000000000e+03 2.127592265605926514e-01 1.853519827127456665e-01 7.536759972572326660e-02 1.273439556825906038e-05 7.786636799573898315e-02 6.503810733556747437e-02 1.316271349787712097e-02 1.853519827127456665e-01 7.536759972572326660e-02 1.273439556825906038e-05 7.786636799573898315e-02 6.503810733556747437e-02
1.000000000000000000e+04 4.572055488824844360e-02 4.245879128575325012e-02 1.474229246377944946e-01 1.034831598190066870e-06 6.012555211782455444e-02 1.185277625918388367e-01 6.008693482726812363e-03 4.245879128575325012e-02 1.474229246377944946e-01 1.034831598190066870e-06 6.012555211782455444e-02 1.185277625918388367e-01
