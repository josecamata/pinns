# learning_rate: 0.002397325940730092
# num_dense_layers: 7
# num_dense_nodes: 120
# activation:sin 
# batch_size: 32
# final loss: 0.3528459966182709
# Training Time: 140.41963052749634
# Best Step: 6000

# step, loss_train, loss_test, metrics_test
0.000000000000000000e+00 6.020640730857849121e-01 1.278393859863281250e+02 5.222497940063476562e+00 6.849758327007293701e-03 7.615618705749511719e-01 3.913930416107177734e+00 5.593528747558593750e-01 1.278393859863281250e+02 5.222497940063476562e+00 6.849758327007293701e-03 7.615618705749511719e-01 3.913930416107177734e+00
1.000000000000000000e+03 1.026998013257980347e-01 2.710528075695037842e-01 4.868641793727874756e-01 1.160703959612874314e-05 3.450960218906402588e-01 3.006520867347717285e-01 2.354492805898189545e-02 2.710528075695037842e-01 4.868641793727874756e-01 1.160703959612874314e-05 3.450960218906402588e-01 3.006520867347717285e-01
2.000000000000000000e+03 9.880059212446212769e-02 4.657296538352966309e-01 2.869248688220977783e-01 7.113941592251649126e-06 2.020464241504669189e-01 1.137013286352157593e-01 1.900179125368595123e-02 4.657296538352966309e-01 2.869248688220977783e-01 7.113941592251649126e-06 2.020464241504669189e-01 1.137013286352157593e-01
3.000000000000000000e+03 1.191425696015357971e-01 1.143050119280815125e-01 3.025260567665100098e-01 4.686712600232567638e-06 2.226639539003372192e-01 1.415164917707443237e-01 8.557767607271671295e-03 1.143050119280815125e-01 3.025260567665100098e-01 4.686712600232567638e-06 2.226639539003372192e-01 1.415164917707443237e-01
4.000000000000000000e+03 9.876501560211181641e-02 2.994850873947143555e-01 2.007849812507629395e-01 2.966698730233474635e-06 1.491827070713043213e-01 8.286976069211959839e-02 3.265328705310821533e-02 2.994850873947143555e-01 2.007849812507629395e-01 2.966698730233474635e-06 1.491827070713043213e-01 8.286976069211959839e-02
5.000000000000000000e+03 5.150764063000679016e-02 7.654818892478942871e-01 7.122743874788284302e-02 3.173674940626369789e-06 5.861572176218032837e-02 7.653527706861495972e-02 1.744471676647663116e-02 7.654818892478942871e-01 7.122743874788284302e-02 3.173674940626369789e-06 5.861572176218032837e-02 7.653527706861495972e-02
6.000000000000000000e+03 3.045502863824367523e-02 8.041562139987945557e-02 9.375718235969543457e-02 1.499066911492263898e-06 6.509073823690414429e-02 1.041848659515380859e-01 9.396095760166645050e-03 8.041562139987945557e-02 9.375718235969543457e-02 1.499066911492263898e-06 6.509073823690414429e-02 1.041848659515380859e-01
7.000000000000000000e+03 7.189545780420303345e-02 5.365169048309326172e-01 6.167010962963104248e-02 4.040922704007243738e-06 4.864427819848060608e-02 5.858258157968521118e-02 1.949194259941577911e-02 5.365169048309326172e-01 6.167010962963104248e-02 4.040922704007243738e-06 4.864427819848060608e-02 5.858258157968521118e-02
8.000000000000000000e+03 1.150759086012840271e-01 7.466560602188110352e-02 1.739489138126373291e-01 1.544320070934190881e-06 1.799546033143997192e-01 1.387283354997634888e-01 1.603221707046031952e-02 7.466560602188110352e-02 1.739489138126373291e-01 1.544320070934190881e-06 1.799546033143997192e-01 1.387283354997634888e-01
9.000000000000000000e+03 1.222644224762916565e-01 1.166430041193962097e-01 4.736721813678741455e-01 5.579208300332538784e-05 3.593243956565856934e-01 3.258960843086242676e-01 3.291692212224006653e-02 1.166430041193962097e-01 4.736721813678741455e-01 5.579208300332538784e-05 3.593243956565856934e-01 3.258960843086242676e-01
1.000000000000000000e+04 1.593901365995407104e-01 2.252309620380401611e-01 1.289988160133361816e-01 5.861627414560643956e-06 1.797710061073303223e-01 8.930621296167373657e-02 2.985832095146179199e-02 2.252309620380401611e-01 1.289988160133361816e-01 5.861627414560643956e-06 1.797710061073303223e-01 8.930621296167373657e-02
