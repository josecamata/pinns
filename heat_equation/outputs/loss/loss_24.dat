# learning_rate: 0.00322224980192136
# num_dense_layers: 10
# num_dense_nodes: 62
# activation:sin 
# batch_size: 32
# final loss: 0.7362173199653625
# Training Time: 125.96893525123596
# Best Step: 5000

# step, loss_train, loss_test, metrics_test
0.000000000000000000e+00 1.009840607643127441e+00 2.853409576416015625e+01 3.938037455081939697e-01 3.943246847484260798e-04 2.271351963281631470e-01 1.155729889869689941e+00 9.850332736968994141e-01 2.853409576416015625e+01 3.938037157058715820e-01 3.943246847484260798e-04 2.271351963281631470e-01 1.155729889869689941e+00
1.000000000000000000e+03 8.665355294942855835e-02 2.216170728206634521e-01 4.429928660392761230e-01 2.469485298206564039e-05 3.492342233657836914e-01 2.893068790435791016e-01 3.099224157631397247e-02 2.216168642044067383e-01 4.429928660392761230e-01 2.469486753398086876e-05 3.492341935634613037e-01 2.893069386482238770e-01
2.000000000000000000e+03 5.996917560696601868e-02 8.493673056364059448e-02 3.273775279521942139e-01 3.864980953949270770e-06 2.605761289596557617e-01 1.432345360517501831e-01 1.436010096222162247e-02 8.493649959564208984e-02 3.273774981498718262e-01 3.864985046675428748e-06 2.605761587619781494e-01 1.432345360517501831e-01
3.000000000000000000e+03 3.126008808612823486e-02 2.587533891201019287e-01 4.535857439041137695e-01 5.809980848425766453e-06 3.385487794876098633e-01 1.923586279153823853e-01 1.598359458148479462e-02 2.587531208992004395e-01 4.535857737064361572e-01 5.809982667415169999e-06 3.385488390922546387e-01 1.923586875200271606e-01
4.000000000000000000e+03 1.235664561390876770e-01 1.397822052240371704e-01 3.756511211395263672e-01 3.438855128479190171e-05 3.358153998851776123e-01 2.792414128780364990e-01 4.303658753633499146e-02 1.397822052240371704e-01 3.756512105464935303e-01 3.438859857851639390e-05 3.358153998851776123e-01 2.792415022850036621e-01
5.000000000000000000e+03 6.589230895042419434e-02 1.130401492118835449e-01 2.507279515266418457e-01 1.020985291688703001e-05 2.387630641460418701e-01 1.229296997189521790e-01 1.074627228081226349e-02 1.130401641130447388e-01 2.507279515266418457e-01 1.020985200739232823e-05 2.387630790472030640e-01 1.229296475648880005e-01
6.000000000000000000e+03 7.864349335432052612e-02 9.691562503576278687e-02 3.460337519645690918e-01 4.744012585433665663e-06 2.886527478694915771e-01 1.829509884119033813e-01 2.792035602033138275e-02 9.691577404737472534e-02 3.460338115692138672e-01 4.744003490486647934e-06 2.886527478694915771e-01 1.829510182142257690e-01
7.000000000000000000e+03 8.330573141574859619e-02 3.442459404468536377e-01 2.383542209863662720e-01 2.067727564281085506e-06 1.519230455160140991e-01 2.557502686977386475e-01 1.817027106881141663e-02 3.442459404468536377e-01 2.383542507886886597e-01 2.067721652565523982e-06 1.519230306148529053e-01 2.557502985000610352e-01
8.000000000000000000e+03 2.981017231941223145e-01 6.091760396957397461e-01 7.689226865768432617e-01 1.580203024786897004e-05 6.351517438888549805e-01 4.847564697265625000e-01 2.808888629078865051e-02 6.091761589050292969e-01 7.689231038093566895e-01 1.580203570483718067e-05 6.351517438888549805e-01 4.847564697265625000e-01
9.000000000000000000e+03 1.009155750274658203e+00 2.794774472713470459e-01 5.876782536506652832e-01 8.322518624481745064e-06 4.160079956054687500e-01 1.584071218967437744e-01 1.976193189620971680e-01 2.794773578643798828e-01 5.876782536506652832e-01 8.322512258018832654e-06 4.160079360008239746e-01 1.584071218967437744e-01
1.000000000000000000e+04 9.702602028846740723e-02 1.445958763360977173e-01 2.971958220005035400e-01 4.902690307062584907e-07 2.278449982404708862e-01 1.237776502966880798e-01 1.808222942054271698e-02 1.445958465337753296e-01 2.971957325935363770e-01 4.902702244180545676e-07 2.278449982404708862e-01 1.237776279449462891e-01
