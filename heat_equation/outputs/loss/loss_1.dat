# learning_rate: 0.005047165763123033
# num_dense_layers: 7
# num_dense_nodes: 57
# activation:sigmoid 
# batch_size: 32
# final loss: 0.8670469522476196
# Training Time: 99.20486855506897
# Best Step: 10000

# step, loss_train, loss_test, metrics_test
0.000000000000000000e+00 3.744226262369920732e-09 3.567726135253906250e+01 3.616939187049865723e-01 2.411521272733807564e-03 2.411518692970275879e-01 6.028189063072204590e-01 3.759262678926234003e-09 3.567726135253906250e+01 3.616939187049865723e-01 2.411521272733807564e-03 2.411518096923828125e-01 6.028189063072204590e-01
1.000000000000000000e+03 1.056662499904632568e-01 3.309245109558105469e-01 4.527920484542846680e-01 3.242784487156313844e-06 3.424273431301116943e-01 3.337591290473937988e-01 2.585458010435104370e-02 3.309243321418762207e-01 4.527919590473175049e-01 3.242779712309129536e-06 3.424273431301116943e-01 3.337590694427490234e-01
2.000000000000000000e+03 3.092877194285392761e-02 1.018259167671203613e+00 3.373413681983947754e-01 7.174997153924778104e-05 2.660817503929138184e-01 1.035120710730552673e-01 2.951312065124511719e-02 1.018259406089782715e+00 3.373413681983947754e-01 7.174997153924778104e-05 2.660817205905914307e-01 1.035120487213134766e-01
3.000000000000000000e+03 2.995993196964263916e-02 2.130355387926101685e-01 3.729137480258941650e-01 7.443168669851729646e-06 2.924816906452178955e-01 1.022454500198364258e-01 2.221905812621116638e-02 2.130357027053833008e-01 3.729137778282165527e-01 7.443191407219273970e-06 2.924816906452178955e-01 1.022454202175140381e-01
4.000000000000000000e+03 4.021926969289779663e-02 1.478172391653060913e-01 3.796327114105224609e-01 2.170872676288126968e-06 2.971054613590240479e-01 8.732808381319046021e-02 1.905492879450321198e-02 1.478173136711120605e-01 3.796327412128448486e-01 2.170883817598223686e-06 2.971054613590240479e-01 8.732813596725463867e-02
5.000000000000000000e+03 2.311803027987480164e-02 8.445410430431365967e-02 4.093279540538787842e-01 3.412824298720806837e-07 3.171775043010711670e-01 1.118348315358161926e-01 2.095270529389381409e-02 8.445416390895843506e-02 4.093280434608459473e-01 3.412838793792616343e-07 3.171775043010711670e-01 1.118348240852355957e-01
6.000000000000000000e+03 2.390455268323421478e-02 2.777312695980072021e-01 3.415008187294006348e-01 1.069886184268398210e-05 2.685436904430389404e-01 7.141880691051483154e-02 2.169626019895076752e-02 2.777314186096191406e-01 3.415007591247558594e-01 1.069888003257801756e-05 2.685437202453613281e-01 7.141879200935363770e-02
7.000000000000000000e+03 1.109147593379020691e-01 1.855330914258956909e-01 3.862665295600891113e-01 2.065654916805215180e-05 3.003672361373901367e-01 6.784372776746749878e-02 1.622749678790569305e-02 1.855329424142837524e-01 3.862664699554443359e-01 2.065655098704155535e-05 3.003672063350677490e-01 6.784377247095108032e-02
8.000000000000000000e+03 3.050130978226661682e-02 1.064539402723312378e-01 3.715305030345916748e-01 1.855599585098843818e-08 2.941713929176330566e-01 7.945763319730758667e-02 2.775653824210166931e-02 1.064540669322013855e-01 3.715304434299468994e-01 1.855557663077433972e-08 2.941713929176330566e-01 7.945762574672698975e-02
9.000000000000000000e+03 4.168442264199256897e-02 1.015635877847671509e-01 3.803511559963226318e-01 2.940507748405707389e-08 2.962348163127899170e-01 7.070831954479217529e-02 1.818907819688320160e-02 1.015635058283805847e-01 3.803512454032897949e-01 2.940513432747593470e-08 2.962348163127899170e-01 7.070826739072799683e-02
1.000000000000000000e+04 2.057686075568199158e-02 1.610976010560989380e-01 3.523966073989868164e-01 6.520858875092017115e-08 2.803488373756408691e-01 5.723784491419792175e-02 2.289296127855777740e-02 1.610974967479705811e-01 3.523966372013092041e-01 6.520846085322773433e-08 2.803488373756408691e-01 5.723780393600463867e-02
