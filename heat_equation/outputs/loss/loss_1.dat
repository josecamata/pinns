# learning_rate: 0.005
# num_dense_layers: 6
# num_dense_nodes: 40
# activation:sigmoid 
# batch_size: 32
# final loss: 0.870795726776123
# Training Time: 86.56709361076355
# Best Step: 8000

# step, loss_train, loss_test, metrics_test
0.000000000000000000e+00 5.701623422282864340e-07 1.306007995605468750e+02 5.695957660675048828e+00 3.797902911901473999e-02 3.797208309173583984e+00 9.491546630859375000e+00 5.701623422282864340e-07 1.306007995605468750e+02 5.695957660675048828e+00 3.797902911901473999e-02 3.797208309173583984e+00 9.491546630859375000e+00
1.000000000000000000e+03 9.036669135093688965e-02 3.169687092304229736e-01 4.284715056419372559e-01 5.394267077463155147e-07 3.325295150279998779e-01 3.212224245071411133e-01 9.036669135093688965e-02 3.169687092304229736e-01 4.284715056419372559e-01 5.394267077463155147e-07 3.325295150279998779e-01 3.212224245071411133e-01
2.000000000000000000e+03 4.123022407293319702e-02 1.260461807250976562e-01 4.011310935020446777e-01 6.602822395507246256e-06 3.099030852317810059e-01 1.911397725343704224e-01 4.123022407293319702e-02 1.260461807250976562e-01 4.011310935020446777e-01 6.602822395507246256e-06 3.099030852317810059e-01 1.911397725343704224e-01
3.000000000000000000e+03 3.383447602391242981e-02 1.062557995319366455e-01 3.913674950599670410e-01 2.626725745358271524e-06 3.046202957630157471e-01 1.488560736179351807e-01 3.383447602391242981e-02 1.062557995319366455e-01 3.913674950599670410e-01 2.626725745358271524e-06 3.046202957630157471e-01 1.488560736179351807e-01
4.000000000000000000e+03 4.651005193591117859e-02 1.007211282849311829e-01 3.927808403968811035e-01 2.692161615414079279e-06 3.089598417282104492e-01 1.237284019589424133e-01 4.651005193591117859e-02 1.007211282849311829e-01 3.927808403968811035e-01 2.692161615414079279e-06 3.089598417282104492e-01 1.237284019589424133e-01
5.000000000000000000e+03 4.812382161617279053e-02 4.370913803577423096e-01 3.180238604545593262e-01 1.317750047746812925e-05 2.530320584774017334e-01 7.452822476625442505e-02 4.812382161617279053e-02 4.370913803577423096e-01 3.180238604545593262e-01 1.317750047746812925e-05 2.530320584774017334e-01 7.452822476625442505e-02
6.000000000000000000e+03 3.303536400198936462e-02 9.721451997756958008e-02 3.771593570709228516e-01 1.341801407761522569e-06 2.986815869808197021e-01 9.676955640316009521e-02 3.303536400198936462e-02 9.721451997756958008e-02 3.771593570709228516e-01 1.341801407761522569e-06 2.986815869808197021e-01 9.676955640316009521e-02
7.000000000000000000e+03 3.429936990141868591e-02 1.270180791616439819e-01 3.576295375823974609e-01 1.171935792854128522e-07 2.851187586784362793e-01 8.335809409618377686e-02 3.429936990141868591e-02 1.270180791616439819e-01 3.576295375823974609e-01 1.171935792854128522e-07 2.851187586784362793e-01 8.335809409618377686e-02
8.000000000000000000e+03 2.854323200881481171e-02 1.156916692852973938e-01 3.609296679496765137e-01 4.861523734689399134e-07 2.891197204589843750e-01 7.651096582412719727e-02 2.854323200881481171e-02 1.156916692852973938e-01 3.609296679496765137e-01 4.861523734689399134e-07 2.891197204589843750e-01 7.651096582412719727e-02
9.000000000000000000e+03 2.852328494191169739e-02 9.428513795137405396e-02 3.744380176067352295e-01 6.460936106122971978e-07 3.000160455703735352e-01 7.942828536033630371e-02 2.852328494191169739e-02 9.428513795137405396e-02 3.744380176067352295e-01 6.460936106122971978e-07 3.000160455703735352e-01 7.942828536033630371e-02
1.000000000000000000e+04 3.186650201678276062e-02 9.921114891767501831e-02 3.905201256275177002e-01 1.078925288311438635e-05 3.138092756271362305e-01 7.578935474157333374e-02 3.186650201678276062e-02 9.921114891767501831e-02 3.905201256275177002e-01 1.078925288311438635e-05 3.138092756271362305e-01 7.578935474157333374e-02
